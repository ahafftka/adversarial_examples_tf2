"""
TODO(summeryue): cite all papers.

Evaluate how inputs generated with the FGSM attack based on a simple vanilla
classifier can influence the accuracy for:
1. A different vanilla MNIST classifier with the same architecture.
2. A different MNIST classifier that is adversarially trained with PGD.

We expect that the attack is a lot less effective on the adversarially trained
classifier comparing to its performance on a vanilla MNIST classifier.

This shows that adversarial training with data generated by a stronger attack
successfully weakens a weaker attack.

We assume that both classifiers to evaluate have been generated inside of the
model folder.
If not, please run setup_classifiers/generate_mnist_vanilla_classifiers.py and
setup_classifiers/generate_mnist_adv_training_classifiers.py first.

"""

import tensorflow as tf

from adversarial_examples_tf2.attackers.fgsm_attacker import FGSMAttacker
from adversarial_examples_tf2.experiments.classifier_utils.mnist_vanilla import get_trained_model, load_existing_model
from adversarial_examples_tf2.experiments.data_utils.mnist import load_mnist


def main():
    # Load MNIST data as tf.data.Dataset.
    batch_size = 32
    _, test_data = load_mnist(batch_size)
    test_features = tf.stack([features for features, _ in test_data])
    test_features = tf.reshape(test_features, [-1, 28, 28])
    test_labels = tf.stack([label for _, label in test_data])
    test_labels = tf.reshape(test_labels, [-1, ])

    # First, create the adversarial examples based on a trained classifier.
    # This classifier has the same architecture and hyper parameters as the
    # classifiers being evaluated.
    attacked_model_path = "../models/mnist/fully_connected/model-1-10"
    attacked_classifier = load_existing_model(attacked_model_path)
    attacker = FGSMAttacker(attacked_classifier, epsilon=0.1)
    perturbed_features = attacker.generate_adversarial_examples(test_features,
                                                                test_labels)
    perturbed_data = tf.data.Dataset.from_tensor_slices(
        (perturbed_features, test_labels))
    perturbed_data = perturbed_data.shuffle(buffer_size=500).batch(
        batch_size)

    # Then, evaluate the performance of a trained vanilla classifier, on both
    # the original test data and perturbed data.
    print("\nStart evaluating the vanilla classifier.")
    eval_vanilla_model_path = "../models/mnist/fully_connected/model-2-10"
    eval_vanilla_classifier = load_existing_model(eval_vanilla_model_path)
    _, eval_vanilla_acc = eval_vanilla_classifier.evaluate(test_data)
    print("Accuracy on test data for vanilla model:", eval_vanilla_acc)
    _, eval_vanilla_perturbed_acc = eval_vanilla_classifier.evaluate(perturbed_data)
    print("Accuracy on perturbed test data for vanilla model:",
        eval_vanilla_perturbed_acc)

    # Then, evaluate the performance of an adversarially trained classifier.
    print("\nStart evaluating the adversarially trained classifier.")
    eval_adv_trained_model_path = "../models/mnist/adv_training_with_pgd_fully_connected/model-1-10"
    eval_adv_trained_classifier = load_existing_model(eval_adv_trained_model_path)
    _, eval_adv_trained_acc = eval_adv_trained_classifier.evaluate(test_data)
    print("Accuracy on test data for adversarially trained model:", eval_adv_trained_acc)
    _, eval_adv_trained_perturbed_acc = eval_adv_trained_classifier.evaluate(perturbed_data)
    print("Accuracy on perturbed test data for adversarially trained model:",
        eval_adv_trained_perturbed_acc)


if __name__ == "__main__":
    main()
